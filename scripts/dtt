#!/usr/bin/env python3
"""
Dev Time Travel (DTT) Vault System
Implements brick-layer snapshot store with immutable bricks arranged in layers.
"""

import os
import sys
import json
import yaml
import hashlib
import time
import tarfile
import gzip
import tempfile
import shutil
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import argparse


class DTTConfig:
    """Configuration manager for DTT Vault"""
    
    def __init__(self, project_root: Path = None):
        self.project_root = project_root or self.find_project_root()
        self.vault_root = self.project_root / ".dtt"
        self.config_path = self.vault_root / "config.yml"
        self.catalog_path = self.vault_root / "index" / "catalog.json"
        self.refcounts_path = self.vault_root / "index" / "refcounts.json"
        self.logs_path = self.vault_root / "logs" / "events.log"
        
    def find_project_root(self) -> Path:
        """Find the project root directory"""
        current = Path.cwd()
        while current != current.parent:
            if (current / ".git").exists() or (current / "agent-profile.yaml").exists():
                return current
            current = current.parent
        return Path.cwd()
        
    def load_config(self) -> Dict[str, Any]:
        """Load DTT configuration"""
        if not self.config_path.exists():
            raise FileNotFoundError(f"DTT vault not initialized. Run 'dtt init' first.")
            
        with open(self.config_path, 'r') as f:
            return yaml.safe_load(f)
            
    def load_catalog(self) -> Dict[str, Any]:
        """Load brick catalog"""
        if not self.catalog_path.exists():
            return {"bricks": {}, "layers": {"layer-0": {}, "layer-1": {}, "layer-2": {}}, 
                   "created": None, "last_updated": None}
                   
        with open(self.catalog_path, 'r') as f:
            return json.load(f)
            
    def save_catalog(self, catalog: Dict[str, Any]):
        """Save brick catalog"""
        catalog["last_updated"] = time.time()
        self.catalog_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.catalog_path, 'w') as f:
            json.dump(catalog, f, indent=2)
            
    def load_refcounts(self) -> Dict[str, Any]:
        """Load reference counts"""
        if not self.refcounts_path.exists():
            return {"refcounts": {}, "last_gc": None}
            
        with open(self.refcounts_path, 'r') as f:
            return json.load(f)
            
    def save_refcounts(self, refcounts: Dict[str, Any]):
        """Save reference counts"""
        with open(self.refcounts_path, 'w') as f:
            json.dump(refcounts, f, indent=2)


class DTTBrick:
    """Represents an immutable content-addressed brick"""
    
    def __init__(self, brick_id: str, layer: str, timestamp: float, 
                 description: str = "", git_commit: str = ""):
        self.brick_id = brick_id
        self.layer = layer
        self.timestamp = timestamp
        self.description = description
        self.git_commit = git_commit
        self.file_hashes = {}
        self.size_bytes = 0
        
    def to_manifest(self) -> Dict[str, Any]:
        """Convert brick to manifest format"""
        return {
            "brick_id": self.brick_id,
            "layer": self.layer,
            "timestamp": self.timestamp,
            "description": self.description,
            "git_commit": self.git_commit,
            "file_hashes": self.file_hashes,
            "size_bytes": self.size_bytes,
            "created": time.time()
        }
        
    @classmethod
    def from_manifest(cls, manifest: Dict[str, Any]) -> 'DTTBrick':
        """Create brick from manifest"""
        brick = cls(
            manifest["brick_id"],
            manifest["layer"], 
            manifest["timestamp"],
            manifest.get("description", ""),
            manifest.get("git_commit", "")
        )
        brick.file_hashes = manifest.get("file_hashes", {})
        brick.size_bytes = manifest.get("size_bytes", 0)
        return brick


class DTTVault:
    """Core DTT Vault implementation"""
    
    def __init__(self, config: DTTConfig):
        self.config = config
        self.vault_config = config.load_config()
        
    def init_vault(self):
        """Initialize the DTT vault structure"""
        print("üèõÔ∏è Initializing Dev Time Travel Vault...")
        
        # Create directory structure
        for layer in ["layer-0", "layer-1", "layer-2"]:
            (self.config.vault_root / "vault" / layer / "bricks").mkdir(parents=True, exist_ok=True)
            (self.config.vault_root / "vault" / layer / "manifests").mkdir(parents=True, exist_ok=True)
            
        (self.config.vault_root / "index").mkdir(parents=True, exist_ok=True)
        (self.config.vault_root / "logs").mkdir(parents=True, exist_ok=True)
        
        # Initialize catalog
        catalog = self.config.load_catalog()
        if catalog["created"] is None:
            catalog["created"] = time.time()
            self.config.save_catalog(catalog)
            
        # Initialize refcounts
        refcounts = self.config.load_refcounts()
        self.config.save_refcounts(refcounts)
        
        self._log_event("vault_init", {"status": "success"})
        print("‚úÖ DTT Vault initialized successfully")
        
    def create_snapshot(self, snapshot_id: str = None, description: str = "") -> str:
        """Create a whole-repo snapshot as an immutable brick"""
        print("üì∏ Creating DTT snapshot...")
        
        if not snapshot_id:
            snapshot_id = f"snapshot-{int(time.time())}"
            
        # Get git information
        git_commit = self._get_git_commit()
        
        # Create content hash for the entire repo
        content_hash = self._create_repo_hash()
        brick_id = content_hash[:16]  # Use first 16 chars of SHA-256
        
        # Check if this content already exists
        catalog = self.config.load_catalog()
        if brick_id in catalog["bricks"]:
            print(f"üì¶ Content already exists as brick {brick_id}")
            self._increment_refcount(brick_id)
            return brick_id
            
        # Create brick
        brick = DTTBrick(brick_id, "layer-0", time.time(), description, git_commit)
        
        # Create archive
        brick_path = self._create_brick_archive(brick)
        brick.size_bytes = brick_path.stat().st_size
        
        # Save manifest
        manifest_path = self.config.vault_root / "vault" / "layer-0" / "manifests" / f"{brick_id}.json"
        with open(manifest_path, 'w') as f:
            json.dump(brick.to_manifest(), f, indent=2)
            
        # Update catalog
        catalog["bricks"][brick_id] = {
            "layer": "layer-0",
            "timestamp": brick.timestamp,
            "size_bytes": brick.size_bytes
        }
        catalog["layers"]["layer-0"][brick_id] = brick.timestamp
        self.config.save_catalog(catalog)
        
        # Update refcounts
        self._increment_refcount(brick_id)
        
        self._log_event("snapshot_created", {
            "brick_id": brick_id,
            "snapshot_id": snapshot_id,
            "layer": "layer-0",
            "size_bytes": brick.size_bytes
        })
        
        print(f"‚úÖ Snapshot created: {brick_id}")
        
        # Check if compaction is needed
        self._check_compaction_thresholds()
        
        return brick_id
        
    def restore_snapshot(self, brick_id: str, force: bool = False) -> bool:
        """Restore a snapshot to a quarantine branch"""
        print(f"üîÑ Restoring snapshot {brick_id}...")
        
        catalog = self.config.load_catalog()
        if brick_id not in catalog["bricks"]:
            print(f"‚ùå Brick {brick_id} not found in vault")
            return False
            
        brick_info = catalog["bricks"][brick_id]
        layer = brick_info["layer"]
        
        # Load manifest
        manifest_path = self.config.vault_root / "vault" / layer / "manifests" / f"{brick_id}.json"
        if not manifest_path.exists():
            print(f"‚ùå Manifest for brick {brick_id} not found")
            return False
            
        with open(manifest_path, 'r') as f:
            manifest = json.load(f)
            
        brick = DTTBrick.from_manifest(manifest)
        
        # Create quarantine branch
        quarantine_branch = f"{self.vault_config['restore']['quarantine_branch_prefix']}{brick_id}"
        
        if not force and self.vault_config['restore']['force_required']:
            print(f"‚ö†Ô∏è  This will restore to quarantine branch: {quarantine_branch}")
            print("   Use --force to proceed with restore")
            return False
            
        # Extract brick archive to temporary location
        brick_path = self.config.vault_root / "vault" / layer / "bricks" / f"{brick_id}.tar.gz"
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # Extract archive
            with tarfile.open(brick_path, 'r:gz') as tar:
                tar.extractall(temp_dir)
                
            # Create and switch to quarantine branch
            try:
                subprocess.run(['git', 'checkout', '-b', quarantine_branch], 
                             check=True, capture_output=True, cwd=self.config.project_root)
                             
                # Copy files (excluding .git and .dtt)
                for item in Path(temp_dir).iterdir():
                    if item.name not in ['.git', '.dtt']:
                        if item.is_dir():
                            shutil.copytree(item, self.config.project_root / item.name, dirs_exist_ok=True)
                        else:
                            shutil.copy2(item, self.config.project_root / item.name)
                            
                print(f"‚úÖ Restored to quarantine branch: {quarantine_branch}")
                
                self._log_event("snapshot_restored", {
                    "brick_id": brick_id,
                    "quarantine_branch": quarantine_branch
                })
                
                return True
                
            except subprocess.CalledProcessError as e:
                print(f"‚ùå Failed to create quarantine branch: {e}")
                return False
                
    def verify_snapshot(self, brick_id: str = None) -> bool:
        """Verify integrity of snapshot(s)"""
        catalog = self.config.load_catalog()
        
        if brick_id:
            return self._verify_single_brick(brick_id, catalog)
        else:
            return self._verify_all_bricks(catalog)
            
    def compact_layer(self, source_layer: str = "layer-0") -> bool:
        """Perform Tetrino Slam compaction"""
        print(f"üß± Performing Tetrino Slam: {source_layer} ‚Üí {self._next_layer(source_layer)}")
        
        catalog = self.config.load_catalog()
        source_bricks = catalog["layers"][source_layer]
        
        if not source_bricks:
            print(f"üì¶ No bricks to compact in {source_layer}")
            return True
            
        target_layer = self._next_layer(source_layer)
        
        # Create combined archive
        combined_id = hashlib.sha256(
            json.dumps(source_bricks, sort_keys=True).encode()
        ).hexdigest()[:16]
        
        print(f"üîÑ Compacting {len(source_bricks)} bricks into {combined_id}")
        
        # Create compacted brick
        compacted_path = self.config.vault_root / "vault" / target_layer / "bricks" / f"{combined_id}.tar.gz"
        manifest_path = self.config.vault_root / "vault" / target_layer / "manifests" / f"{combined_id}.json"
        
        # Combine all source bricks into one archive
        with tarfile.open(compacted_path, 'w:gz') as combined_tar:
            for brick_id in source_bricks:
                source_path = self.config.vault_root / "vault" / source_layer / "bricks" / f"{brick_id}.tar.gz"
                if source_path.exists():
                    combined_tar.add(source_path, arcname=f"brick_{brick_id}.tar.gz")
                    
        # Create combined manifest
        combined_manifest = {
            "brick_id": combined_id,
            "layer": target_layer,
            "timestamp": time.time(),
            "description": f"Compacted from {source_layer} ({len(source_bricks)} bricks)",
            "source_bricks": list(source_bricks.keys()),
            "size_bytes": compacted_path.stat().st_size,
            "created": time.time()
        }
        
        with open(manifest_path, 'w') as f:
            json.dump(combined_manifest, f, indent=2)
            
        # Update catalog
        catalog["bricks"][combined_id] = {
            "layer": target_layer,
            "timestamp": combined_manifest["timestamp"],
            "size_bytes": combined_manifest["size_bytes"]
        }
        catalog["layers"][target_layer][combined_id] = combined_manifest["timestamp"]
        
        # Remove source bricks
        source_brick_ids = list(source_bricks.keys())
        for brick_id in source_brick_ids:
            self._remove_brick(brick_id, source_layer, catalog)
            
        catalog["layers"][source_layer] = {}
        self.config.save_catalog(catalog)
        
        self._log_event("layer_compacted", {
            "source_layer": source_layer,
            "target_layer": target_layer,
            "combined_id": combined_id,
            "source_count": len(source_bricks)
        })
        
        print(f"‚úÖ Compaction complete: {combined_id}")
        return True
        
    def prune_vault(self) -> bool:
        """Prune vault according to retention policy"""
        print("üßπ Pruning vault according to retention policy...")
        
        config = self.vault_config
        catalog = self.config.load_catalog()
        refcounts = self.config.load_refcounts()
        now = time.time()
        
        pruned_count = 0
        
        # Layer-0 pruning (days)
        cutoff_layer0 = now - (config["retention"]["layer0_days"] * 24 * 3600)
        for brick_id, timestamp in list(catalog["layers"]["layer-0"].items()):
            if timestamp < cutoff_layer0 and refcounts["refcounts"].get(brick_id, 0) <= 1:
                self._remove_brick(brick_id, "layer-0", catalog)
                pruned_count += 1
                
        # Layer-1 pruning (weeks)
        cutoff_layer1 = now - (config["retention"]["layer1_weeks"] * 7 * 24 * 3600)
        for brick_id, timestamp in list(catalog["layers"]["layer-1"].items()):
            if timestamp < cutoff_layer1 and refcounts["refcounts"].get(brick_id, 0) <= 1:
                self._remove_brick(brick_id, "layer-1", catalog)
                pruned_count += 1
                
        # Layer-2 pruning (months)
        cutoff_layer2 = now - (config["retention"]["layer2_months"] * 30 * 24 * 3600)
        for brick_id, timestamp in list(catalog["layers"]["layer-2"].items()):
            if timestamp < cutoff_layer2 and refcounts["refcounts"].get(brick_id, 0) <= 1:
                self._remove_brick(brick_id, "layer-2", catalog)
                pruned_count += 1
                
        self.config.save_catalog(catalog)
        refcounts["last_gc"] = now
        self.config.save_refcounts(refcounts)
        
        self._log_event("vault_pruned", {"pruned_count": pruned_count})
        print(f"‚úÖ Pruned {pruned_count} bricks")
        return True
        
    def _create_repo_hash(self) -> str:
        """Create SHA-256 hash of current repo state"""
        hasher = hashlib.sha256()
        
        # Include git commit if available
        git_commit = self._get_git_commit()
        hasher.update(git_commit.encode())
        
        # Hash all tracked files
        try:
            # Get list of tracked files
            result = subprocess.run(['git', 'ls-files'], 
                                  capture_output=True, text=True, cwd=self.config.project_root)
            if result.returncode == 0:
                files = result.stdout.strip().split('\n')
                for file_path in sorted(files):
                    if file_path:
                        full_path = self.config.project_root / file_path
                        if full_path.exists() and full_path.is_file():
                            with open(full_path, 'rb') as f:
                                hasher.update(f.read())
        except Exception:
            # Fallback: hash all files in project
            for file_path in sorted(self.config.project_root.rglob('*')):
                if (file_path.is_file() and 
                    not any(part.startswith('.') for part in file_path.parts[len(self.config.project_root.parts):]) and
                    '.dtt' not in str(file_path)):
                    try:
                        with open(file_path, 'rb') as f:
                            hasher.update(f.read())
                    except Exception:
                        continue
                        
        return hasher.hexdigest()
        
    def _create_brick_archive(self, brick: DTTBrick) -> Path:
        """Create compressed archive for brick"""
        brick_path = self.config.vault_root / "vault" / brick.layer / "bricks" / f"{brick.brick_id}.tar.gz"
        brick_path.parent.mkdir(parents=True, exist_ok=True)
        
        with tarfile.open(brick_path, 'w:gz') as tar:
            # Add all files except .git and .dtt
            for item in self.config.project_root.iterdir():
                if item.name not in ['.git', '.dtt']:
                    tar.add(item, arcname=item.name)
                    
        return brick_path
        
    def _get_git_commit(self) -> str:
        """Get current git commit hash"""
        try:
            result = subprocess.run(['git', 'rev-parse', 'HEAD'], 
                                  capture_output=True, text=True, check=True, 
                                  cwd=self.config.project_root)
            return result.stdout.strip()
        except Exception:
            return "unknown"
            
    def _increment_refcount(self, brick_id: str):
        """Increment reference count for brick"""
        refcounts = self.config.load_refcounts()
        refcounts["refcounts"][brick_id] = refcounts["refcounts"].get(brick_id, 0) + 1
        self.config.save_refcounts(refcounts)
        
    def _remove_brick(self, brick_id: str, layer: str, catalog: Dict[str, Any]):
        """Remove brick from storage"""
        brick_path = self.config.vault_root / "vault" / layer / "bricks" / f"{brick_id}.tar.gz"
        manifest_path = self.config.vault_root / "vault" / layer / "manifests" / f"{brick_id}.json"
        
        if brick_path.exists():
            brick_path.unlink()
        if manifest_path.exists():
            manifest_path.unlink()
            
        if brick_id in catalog["bricks"]:
            del catalog["bricks"][brick_id]
        if brick_id in catalog["layers"][layer]:
            del catalog["layers"][layer][brick_id]
            
    def _next_layer(self, current_layer: str) -> str:
        """Get next layer for compaction"""
        layer_map = {"layer-0": "layer-1", "layer-1": "layer-2"}
        return layer_map.get(current_layer, "layer-2")
        
    def _check_compaction_thresholds(self):
        """Check if compaction is needed based on thresholds"""
        config = self.vault_config
        catalog = self.config.load_catalog()
        
        # Check Layer-0 thresholds
        layer0_bricks = catalog["layers"]["layer-0"]
        if len(layer0_bricks) >= config["thresholds"]["layer0_max_bricks"]:
            print("üö® Layer-0 brick threshold reached - triggering compaction")
            self.compact_layer("layer-0")
            
        # Could also check size thresholds here
        
    def _verify_single_brick(self, brick_id: str, catalog: Dict[str, Any]) -> bool:
        """Verify integrity of a single brick"""
        if brick_id not in catalog["bricks"]:
            print(f"‚ùå Brick {brick_id} not found")
            return False
            
        brick_info = catalog["bricks"][brick_id]
        layer = brick_info["layer"]
        
        brick_path = self.config.vault_root / "vault" / layer / "bricks" / f"{brick_id}.tar.gz"
        manifest_path = self.config.vault_root / "vault" / layer / "manifests" / f"{brick_id}.json"
        
        if not brick_path.exists():
            print(f"‚ùå Brick file missing: {brick_id}")
            return False
            
        if not manifest_path.exists():
            print(f"‚ùå Manifest missing: {brick_id}")
            return False
            
        print(f"‚úÖ Brick {brick_id} verified")
        return True
        
    def _verify_all_bricks(self, catalog: Dict[str, Any]) -> bool:
        """Verify integrity of all bricks"""
        print("üîç Verifying all bricks...")
        
        all_valid = True
        for brick_id in catalog["bricks"]:
            if not self._verify_single_brick(brick_id, catalog):
                all_valid = False
                
        if all_valid:
            print("‚úÖ All bricks verified successfully")
        else:
            print("‚ùå Some bricks failed verification")
            
        return all_valid
        
    def _log_event(self, event_type: str, data: Dict[str, Any]):
        """Log vault event"""
        self.config.logs_path.parent.mkdir(parents=True, exist_ok=True)
        
        log_entry = {
            "timestamp": time.time(),
            "event_type": event_type,
            "data": data
        }
        
        with open(self.config.logs_path, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')


def main():
    """Main CLI entry point"""
    parser = argparse.ArgumentParser(description="Dev Time Travel Vault - Brick-Layer Snapshot Store")
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Init command
    init_parser = subparsers.add_parser('init', help='Initialize DTT vault')
    
    # Snapshot command
    snapshot_parser = subparsers.add_parser('snapshot', help='Create snapshot')
    snapshot_parser.add_argument('--id', help='Snapshot identifier')
    snapshot_parser.add_argument('--description', help='Snapshot description', default="")
    
    # Restore command
    restore_parser = subparsers.add_parser('restore', help='Restore snapshot')
    restore_parser.add_argument('brick_id', help='Brick ID to restore')
    restore_parser.add_argument('--force', action='store_true', help='Force restore')
    
    # Verify command
    verify_parser = subparsers.add_parser('verify', help='Verify snapshot integrity')
    verify_parser.add_argument('brick_id', nargs='?', help='Brick ID to verify (or "all")')
    
    # Compact command
    compact_parser = subparsers.add_parser('compact', help='Compact layer')
    compact_parser.add_argument('--layer', default='layer-0', help='Layer to compact')
    
    # Prune command
    prune_parser = subparsers.add_parser('prune', help='Prune according to retention policy')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
        
    # Initialize DTT system
    config = DTTConfig()
    
    if args.command == 'init':
        # Create default config first
        config.vault_root.mkdir(parents=True, exist_ok=True)
        default_config = {
            "retention": {"layer0_days": 14, "layer1_weeks": 12, "layer2_months": 24},
            "thresholds": {"layer0_max_gb": 4, "layer0_max_bricks": 500, "layer1_max_gb": 16, "layer1_max_bricks": 100},
            "integrity": {"hash": "sha256", "sign": True},
            "restore": {"quarantine_branch_prefix": "dtt/reconstruct/", "force_required": True}
        }
        with open(config.config_path, 'w') as f:
            yaml.dump(default_config, f, default_flow_style=False)
        
        vault = DTTVault(config)
        vault.init_vault()
        return 0
        
    # For other commands, check if vault exists
    if not config.vault_root.exists():
        print("‚ùå DTT vault not initialized. Run 'dtt init' first.")
        return 1
        
    vault = DTTVault(config)
    
    try:
        if args.command == 'snapshot':
            vault.create_snapshot(args.id, args.description)
        elif args.command == 'restore':
            if not vault.restore_snapshot(args.brick_id, args.force):
                return 1
        elif args.command == 'verify':
            if args.brick_id == 'all':
                if not vault.verify_snapshot():
                    return 1
            else:
                if not vault.verify_snapshot(args.brick_id):
                    return 1
        elif args.command == 'compact':
            if not vault.compact_layer(args.layer):
                return 1
        elif args.command == 'prune':
            vault.prune_vault()
        else:
            print(f"‚ùå Unknown command: {args.command}")
            return 1
            
        return 0
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())