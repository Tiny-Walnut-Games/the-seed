# TLDA-Warbler to Ollama Bridge - Simplified Integration Without Docker
# Direct Ollama binary integration for simplified user experience

import json
import os
import time
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
import sys

# Add scripts directory to path for OllamaManager import
scripts_dir = Path(__file__).parent
sys.path.insert(0, str(scripts_dir))

try:
    from ollama_manager import OllamaManager
except ImportError:
    print("âŒ Failed to import OllamaManager. Please ensure ollama_manager.py is available.")
    OllamaManager = None

class WarblerGemma3Bridge:
    """Simplified production bridge between TLDA/Warbler and direct Ollama integration"""
    
    def __init__(self, model_name="llama3.2:1b", max_tokens=512, temperature=0.7):
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.conversation_history = []
        self.last_response_time = 0
        
        # Initialize Ollama manager (no Docker required)
        self.ollama_manager = OllamaManager() if OllamaManager else None
        self.ollama_available = self.ollama_manager is not None
        self.model_available = False
        
        if self.ollama_available:
            self._initialize_ollama()
    
    def _initialize_ollama(self):
        """Initialize Ollama with automatic startup and model management"""
        print("ðŸ”§ Initializing Ollama (no Docker required)...")
        
        if not self.ollama_manager:
            print("âš ï¸ OllamaManager not available - using enhanced stubs")
            self.ollama_available = False
            return
            
        if not self.ollama_manager.ollama_path:
            print("âš ï¸ Ollama binary not available - using enhanced stubs")
            self.ollama_available = False
            return
        
        # Start Ollama server if not running
        if not self.ollama_manager.is_running():
            print("ðŸš€ Starting Ollama server...")
            if not self.ollama_manager.start():
                print("âŒ Failed to start Ollama server - using enhanced stubs")
                self.ollama_available = False
                return
        
        # Ensure model is available
        models = self.ollama_manager.list_models()
        if self.model_name not in models:
            print(f"ðŸ“¥ Downloading model {self.model_name}...")
            if self.ollama_manager.ensure_model(self.model_name):
                self.model_available = True
                print(f"âœ… Model {self.model_name} ready")
            else:
                print(f"âŒ Failed to download model {self.model_name}")
                # Try fallback models
                fallback_models = ["llama3.2:1b", "llama2", "phi3:mini"]
                for fallback in fallback_models:
                    if fallback in models or self.ollama_manager.ensure_model(fallback):
                        self.model_name = fallback
                        self.model_available = True
                        print(f"âœ… Using fallback model: {fallback}")
                        break
        else:
            self.model_available = True
            print(f"âœ… Model {self.model_name} already available")
    
    def _check_docker_availability(self) -> bool:
        """DEPRECATED: Docker no longer required"""
        print("â„¹ï¸ Docker dependency removed - using direct Ollama binary")
        return False
    
    def _check_model_availability(self) -> bool:
        """DEPRECATED: Using direct Ollama model management"""
        return self.model_available
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get comprehensive system status for diagnostics"""
        if not self.ollama_available or not self.ollama_manager:
            return {
                "ollama_available": False,
                "model_available": False,
                "ready_for_inference": False,
                "error": "OllamaManager not available"
            }
        
        status = self.ollama_manager.get_status()
        return {
            "ollama_available": True,
            "model_available": self.model_available,
            "ready_for_inference": status["server_running"] and self.model_available,
            "server_running": status["server_running"],
            "available_models": status["available_models"],
            "current_model": self.model_name,
            "endpoint": status["endpoint"]
        }
    def send_prompt(self, prompt: str, context: str = "", system_prompt: str = "") -> str:
        """Send a prompt to Ollama with simplified direct integration"""
        if not self.ollama_available or not self.ollama_manager:
            return self._generate_enhanced_stub_response(prompt, context)
        
        if not self.ollama_manager.is_running():
            print("ðŸ”§ Ollama not running, attempting to start...")
            if not self.ollama_manager.start():
                return self._generate_enhanced_stub_response(prompt, context, "Failed to start Ollama server")
        
        if not self.model_available:
            print(f"ðŸ“¥ Model {self.model_name} not available, attempting to download...")
            if not self.ollama_manager.ensure_model(self.model_name):
                return self._generate_enhanced_stub_response(prompt, context, f"Model {self.model_name} unavailable")
            self.model_available = True
        
        start_time = time.time()
        
        try:
            # Prepare the full prompt with context
            full_prompt = prompt
            if context:
                full_prompt = f"Context: {context}\n\nQuery: {prompt}"
            if system_prompt:
                full_prompt = f"System: {system_prompt}\n\n{full_prompt}"
            
            print(f"ðŸ§™â€â™‚ï¸ Sending to Ollama: {full_prompt[:100]}...")
            
            # Use Ollama API for generation
            response_data = self.ollama_manager.generate(
                model=self.model_name,
                prompt=full_prompt,
                stream=False
            )
            
            if "error" in response_data:
                print(f"âŒ Ollama error: {response_data['error']}")
                return self._generate_enhanced_stub_response(prompt, context, response_data['error'])
            
            response = response_data.get('response', '').strip()
            if not response:
                print("âŒ Empty response from Ollama")
                return self._generate_enhanced_stub_response(prompt, context, "Empty response")
            
            self.last_response_time = time.time() - start_time
            print(f"âœ… Ollama response received ({len(response)} chars, {self.last_response_time:.2f}s)")
            
            # Store in conversation history
            self.conversation_history.append({
                "timestamp": time.time(),
                "prompt": prompt,
                "context": context,
                "response": response,
                "response_time": self.last_response_time,
                "model": self.model_name
            })
            
            return response
                
        except Exception as e:
            self.last_response_time = time.time() - start_time
            error_msg = f"Error connecting to Ollama: {str(e)}"
            print(f"âŒ {error_msg}")
            return self._generate_enhanced_stub_response(prompt, context, error_msg)
    
    def _generate_enhanced_stub_response(self, prompt: str, context: str = "", error: str = "") -> str:
        """Generate enhanced stub responses when AI is unavailable"""
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        
        # Basic template matching for different prompt types
        response_templates = {
            "decision": {
                "keywords": ["decision", "choose", "recommend", "suggest", "option"],
                "response": """**Warbler Decision Assistance (Enhanced Stub Mode)**

Based on the prompt analysis, here are recommended considerations:

1. **Evaluate Options**: Review each alternative systematically
2. **Consider Context**: Factor in project constraints and goals  
3. **Risk Assessment**: Identify potential challenges for each option
4. **Timeline Impact**: Consider development time implications
5. **Team Expertise**: Match options with available skills

**Next Steps**: 
- Prototype key components
- Validate assumptions with testing
- Seek team input on implementation approach

*Note: Enhanced stub response generated due to AI unavailability. Full AI analysis available when Ollama is configured.*"""
            },
            "code_analysis": {
                "keywords": ["analyze", "script", "code", "review", "unity", "c#"],
                "response": """**Unity Script Analysis (Enhanced Stub Mode)**

**Code Quality Checklist**:
âœ“ Variable naming follows conventions
âœ“ Methods have clear single responsibilities  
âœ“ Performance considerations for Update loops
âœ“ Proper Unity component usage
âœ“ Error handling and null checks

**Common Optimizations**:
- Cache frequently accessed components
- Use object pooling for instantiated objects
- Minimize allocations in Update/FixedUpdate
- Consider using Events for loose coupling

**Unity Best Practices**:
- Use SerializeField for private inspector variables
- Implement proper cleanup in OnDestroy
- Follow Unity component lifecycle patterns

*Note: Enhanced stub analysis. Full AI-powered code review available when Ollama is configured.*"""
            },
            "tldl": {
                "keywords": ["tldl", "log", "document", "entry"],
                "response": f"""**TLDL Entry Generation (Enhanced Stub Mode)**

## Development Activity Summary
**Timestamp**: {timestamp}
**Context**: {context or 'General development work'}

### Work Completed
- Implementation progress on requested features
- Code structure and architecture decisions
- Testing and validation activities

### Technical Decisions
- Architecture patterns selected based on best practices
- Technology choices aligned with project goals
- Performance and maintainability considerations

### Lessons Learned
- Development workflow insights
- Technical challenge solutions
- Process improvement opportunities

### Next Steps
- Continue implementation following established patterns
- Expand testing coverage
- Documentation updates

*Note: Enhanced stub entry. Full AI-generated TLDL entries available when Ollama is configured.*"""
            }
        }
        
        # Determine response type based on keywords
        prompt_lower = prompt.lower()
        context_lower = context.lower()
        combined_text = f"{prompt_lower} {context_lower}"
        
        selected_template = None
        for template_type, template_data in response_templates.items():
            if any(keyword in combined_text for keyword in template_data["keywords"]):
                selected_template = template_data["response"]
                print(f"ðŸ¤– Using enhanced stub template: {template_type}")
                break
        
        # Default general response
        if not selected_template:
            selected_template = f"""**Warbler AI Assistant (Enhanced Stub Mode)**

I understand you're asking about: "{prompt[:100]}..."

**Enhanced Response Generation**:
Based on the prompt analysis, this appears to be a request for development assistance. While full AI capabilities are temporarily unavailable, here's a structured approach:

**Immediate Actions**:
1. Break down the request into manageable components
2. Identify existing patterns or templates that apply
3. Consider project constraints and requirements
4. Plan implementation steps

**Context Considerations**: {context or 'General development context'}

**Recommended Next Steps**:
- Review existing codebase for similar implementations
- Consult documentation and best practices
- Plan incremental development approach
- Set up testing and validation

*Note: This is an enhanced stub response. Full AI capabilities available when Ollama is properly configured.*

**System Status**: {error or 'AI temporarily unavailable'}
**Timestamp**: {timestamp}"""
        
        # Add error information if provided
        if error:
            selected_template += f"\n\n**Technical Details**: {error}"
            selected_template += "\n\n**Resolution**: Install Ollama binary to enable full AI capabilities."
        
        return selected_template
    def warbler_decision_assist(self, decision_context, options):
        """Use Ollama to assist Warbler's decision making"""
        prompt = f"""
        As a decision-making assistant for the Warbler cognitive architecture:
        
        Context: {decision_context}
        Options: {', '.join(options)}
        
        Please provide:
        1. Recommended option with rationale
        2. Potential risks for each option
        3. Suggested next steps
        
        Keep response focused and actionable for development workflow.
        """
        
        return self.send_prompt(prompt, context="Warbler Decision Support")
    
    def analyze_unity_script(self, script_content: str, analysis_type: str = "general") -> str:
        """Analyze Unity C# scripts for optimization and improvements"""
        prompt = f"""
        Analyze this Unity C# script for {analysis_type} improvements:
        
        ```csharp
        {script_content}
        ```
        
        Provide specific recommendations for:
        1. Performance optimizations
        2. Code quality improvements
        3. Unity best practices
        4. Potential bugs or issues
        """
        
        return self.send_prompt(prompt, context="Unity Script Analysis")
    
    def generate_tldl_entry(self, dev_activity: str, outcomes: List[str]) -> str:
        """Generate TLDL (The Living Dev Log) entries"""
        prompt = f"""
        Generate a TLDL entry for this development activity:
        
        Activity: {dev_activity}
        Outcomes: {', '.join(outcomes)}
        
        Create a structured TLDL entry with:
        1. Clear title
        2. Summary of work completed
        3. Technical decisions made
        4. Lessons learned
        5. Next steps identified
        
        Format as markdown suitable for documentation.
        """
        
        return self.send_prompt(prompt, context="TLDL Generation")
    
    def warbler_code_review(self, code_diff: str, file_path: str) -> str:
        """Perform AI-assisted code review"""
        prompt = f"""
        Perform a code review for this change:
        
        File: {file_path}
        
        ```diff
        {code_diff}
        ```
        
        Focus on:
        1. Code quality and maintainability
        2. Potential bugs or security issues
        3. Performance implications
        4. Adherence to coding standards
        5. Suggestions for improvement
        """
        
        return self.send_prompt(prompt, context="Code Review")
    
    def batch_process_prompts(self, prompts: List[Dict[str, str]]) -> List[str]:
        """Process multiple prompts efficiently"""
        results = []
        
        for i, prompt_data in enumerate(prompts):
            print(f"ðŸ”„ Processing batch item {i+1}/{len(prompts)}")
            
            prompt = prompt_data.get("prompt", "")
            context = prompt_data.get("context", "")
            
            response = self.send_prompt(prompt, context)
            results.append(response)
            
            # Rate limiting to avoid overwhelming the model
            if i < len(prompts) - 1:  # Don't sleep after the last item
                time.sleep(1)
        
        return results
    
    def save_conversation_history(self, file_path: str):
        """Save conversation history to file"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "model": self.model_name,
                    "timestamp": time.time(),
                    "conversations": self.conversation_history
                }, f, indent=2)
            print(f"ðŸ’¾ Conversation history saved to {file_path}")
        except Exception as e:
            print(f"âŒ Failed to save conversation history: {e}")
    
    def load_conversation_history(self, file_path: str):
        """Load conversation history from file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.conversation_history = data.get("conversations", [])
            print(f"ðŸ“‚ Conversation history loaded from {file_path}")
        except Exception as e:
            print(f"âŒ Failed to load conversation history: {e}")

# Comprehensive test suite
if __name__ == "__main__":
    print("ðŸ§™â€â™‚ï¸ TLDA-Warbler-Ollama Bridge - Simplified Integration Test Suite (No Docker Required)")
    print("=" * 80)
    
    # Initialize bridge
    bridge = WarblerGemma3Bridge()
    
    # System status check
    status = bridge.get_system_status()
    print(f"ðŸ“Š System Status:")
    print(f"   Ollama Available: {'âœ…' if status['ollama_available'] else 'âŒ'}")
    print(f"   Model Available: {'âœ…' if status['model_available'] else 'âŒ'}")
    print(f"   Ready for Inference: {'âœ…' if status['ready_for_inference'] else 'âŒ'}")
    if 'available_models' in status:
        print(f"   Available Models: {status['available_models']}")
    if 'current_model' in status:
        print(f"   Current Model: {status['current_model']}")
    print()
    
    if not status['ready_for_inference']:
        print("âš ï¸  System not ready, but enhanced stubs will be used.")
        print("   For full AI capabilities:")
        print("   1. Ollama will auto-download and start when needed")
        print("   2. No Docker installation required")
        print("   3. Models downloaded automatically on first use")
        print()
    
    # Test 1: Basic communication (will use enhanced stub if AI unavailable)
    print("ðŸ§ª Test 1: Basic Communication")
    response = bridge.send_prompt("Hello from TLDA! Can you help with development decisions?")
    print(f"Response: {response[:200]}...")
    print()
    
    # Test 2: Warbler decision assistance
    print("ðŸ§ª Test 2: Warbler Decision Assistance")
    decision_response = bridge.warbler_decision_assist(
        "Need to choose architecture for NPC controller",
        ["Component-based ECS", "State machine", "Behavior trees"]
    )
    print(f"Decision assistance: {decision_response[:200]}...")
    print()
    
    # Test 3: Unity script analysis
    print("ðŸ§ª Test 3: Unity Script Analysis")
    sample_script = '''
    public class PlayerController : MonoBehaviour 
    {
        void Update() 
        {
            transform.Translate(Vector3.forward * Time.deltaTime);
        }
    }
    '''
    analysis = bridge.analyze_unity_script(sample_script, "performance")
    print(f"Script analysis: {analysis[:200]}...")
    print()
    
    # Test 4: TLDL generation
    print("ðŸ§ª Test 4: TLDL Entry Generation")
    tldl_entry = bridge.generate_tldl_entry(
        "Implemented simplified Ollama integration", 
        ["Direct binary integration", "Enhanced stub responses", "No Docker dependency"]
    )
    print(f"TLDL entry: {tldl_entry[:200]}...")
    print()
    
    # Test 5: Enhanced stub response when AI unavailable
    print("ðŸ§ª Test 5: Enhanced Stub Response System")
    if not status['ready_for_inference']:
        print("âœ… Enhanced stubs active - providing intelligent fallback responses")
    else:
        print("âœ… Full AI available - enhanced stubs ready as fallback")
    print()
    
    # Save conversation history
    print("ðŸ’¾ Saving conversation history...")
    bridge.save_conversation_history("warbler_ollama_conversation_log.json")
    
    print("âœ… All tests completed! Bridge is ready for production use.")
    print(f"ðŸ“ˆ Total conversation exchanges: {len(bridge.conversation_history)}")
    print(f"âš¡ Last response time: {bridge.last_response_time:.2f}s")
    print()
    print("ðŸŽ¯ Key Benefits:")
    print("   âœ… No Docker dependency required")
    print("   âœ… Automatic Ollama binary management")  
    print("   âœ… Enhanced stub responses when AI unavailable")
    print("   âœ… One-click setup experience")
    print("   âœ… Graceful fallbacks and error handling")
